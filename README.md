# Data-Analysis-Pipeline-using-Apache-Spark

In this project, we learn how data analysis and performance could be improved using frameworks like Apache Spark.

In preparation phase, we get familiar with Spark environment by working on sample programs like word count.
Later, we work on Titanic data set to perform data analysis with Machine learning libraries like MLlib and predict the labels of test data set.

In the final part of the project, we build a data pipeline using
a. Data from sources such as NY Times articles using the APIs provided by the data sources.
b. Split the data into training and test data set.
c. Extract features that will determine the class or category of the article {politics, sports,
business, Technology}
d. Build a model for classification using any two of the several classification algorithms.
e. Assess the accuracy using a query text or new article for each news “category”
f. Compare the classification accuracy of at least two well-known classification algorithms,
for a given text data set.

In the end, we document the design and implementation of the project. 
The knowledge and skills gained in this exercise could be used to solve classification problems in other domains.
