{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code does Multi-class text classification using pyspark.\n",
    "Article text is preprocessed by removing non-alphanumeric characters. \n",
    "Tokenizing, Stopwords removal and Count vector operations are done with a pipeline.\n",
    "Training is done with the complete dataset without any split.\n",
    "Unknown articles from different topics are collected.\n",
    "Trained models are used to predict the class of the article.\n",
    "\n",
    "Predictions are made the following ways\n",
    "1. Logistic Regression using Count Vector Features\n",
    "2. Logistic Regression using TF-IDF Features\n",
    "3. Cross-Validation\n",
    "4. Naive Bayes\n",
    "5. Random Forest\n",
    "\n",
    "Sample output: Accuracy\n",
    "1. 0.73\n",
    "2. 0.75\n",
    "3. 0.87\n",
    "4. 0.80\n",
    "5. 0.93"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os, csv, re\n",
    "\n",
    "def preProcessData():\n",
    "    with open('UnknownArticles.csv', 'w+') as csvfile:\n",
    "        fieldnames = ['text', 'category']\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "\n",
    "        writer.writeheader()\n",
    "\n",
    "        keyWords = ['blockchain','business','sports','politics']\n",
    "\n",
    "        for keyWord in keyWords:\n",
    "\n",
    "            for i in range(50, 60):\n",
    "                total_path = \"/Users/sajidkhan/Desktop/DIC/Lab3/Input/unknown/\"+keyWord\n",
    "                text_file = open(os.path.join(total_path, keyWord+str(i)+\".txt\"), \"r\")\n",
    "                for line in text_file:\n",
    "                    line2 = re.sub('[^A-Za-z0-9\\s]+', '', line)\n",
    "                    writer.writerow({'text': line2, 'category': keyWord})  \n",
    "\n",
    "                text_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Dataset Count: 200\n",
      "Test Dataset Count: 40\n",
      "+------------------------------+----------+------------------------------+-----+----------+\n",
      "|                          text|  category|                   probability|label|prediction|\n",
      "+------------------------------+----------+------------------------------+-----+----------+\n",
      "|The distance between the Wh...|blockchain|[0.9125202701036145,0.01689...|  0.0|       0.0|\n",
      "|Instead of perfectly polish...|blockchain|[0.9125202701036145,0.01689...|  0.0|       0.0|\n",
      "|There are now more ways to ...|blockchain|[0.9125202701036145,0.01689...|  0.0|       0.0|\n",
      "|The pioneers of social medi...|blockchain|[0.9125202701036145,0.01689...|  0.0|       0.0|\n",
      "|The internet was born in te...|blockchain|[0.9125202701036145,0.01689...|  0.0|       0.0|\n",
      "|Major companies have been f...|blockchain|[0.911839091049913,0.016619...|  0.0|       0.0|\n",
      "|AdvertisementSupported byTh...|blockchain|[0.8901288681191811,0.03274...|  0.0|       0.0|\n",
      "|AdvertisementSupported byTO...|  business|[0.633277423020905,0.259481...|  1.0|       0.0|\n",
      "|AdvertisementSupported byKU...|  politics|[0.569913183058957,0.133241...|  2.0|       0.0|\n",
      "|AdvertisementSupported byWA...|  business|[0.5299657263786663,0.28228...|  1.0|       0.0|\n",
      "+------------------------------+----------+------------------------------+-----+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.728374480696462"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model Training and Evaluation\n",
    "# Logistic Regression using Count Vector Features\n",
    "\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark import SparkContext\n",
    "import pyspark\n",
    "import pandas as pd\n",
    "\n",
    "sc = pyspark.SparkContext.getOrCreate()\n",
    "sqlContext = SQLContext(sc)\n",
    "preProcessData()\n",
    "trainingFile = \"/Users/sajidkhan/Desktop/DIC/Lab3/DataAnalysisCode/ArticleData.csv\"\n",
    "trainingData = sqlContext.read.format('com.databricks.spark.csv').options(header='true', inferschema='true').load(trainingFile)\n",
    "\n",
    "testFile = \"/Users/sajidkhan/Desktop/DIC/Lab3/DataAnalysisCode/UnknownArticles.csv\"\n",
    "testData = sqlContext.read.format('com.databricks.spark.csv').options(header='true', inferschema='true').load(testFile)\n",
    "\n",
    "from pyspark.ml.feature import RegexTokenizer, StopWordsRemover, CountVectorizer\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# regular expression tokenizer\n",
    "regexTokenizer = RegexTokenizer(inputCol=\"text\", outputCol=\"words\", pattern=\"\\\\W\")\n",
    "\n",
    "# stop words\n",
    "add_stopwords = [\"a\", \"an\", \"as\", \"able\", \"about\", \"above\", \"according\", \"accordingly\", \"across\", \"actually\", \"after\", \"afterwards\", \"again\", \"against\", \"aint\", \"all\", \"allow\", \"allows\", \"almost\", \"alone\", \"along\", \"already\", \"also\", \"although\", \"always\", \"am\", \"among\", \"amongst\", \"an\", \"and\", \"another\", \"any\", \"anybody\", \"anyhow\", \"anyone\", \"anything\", \"anyway\", \"anyways\", \"anywhere\", \"apart\", \"appear\", \"appreciate\", \"appropriate\", \"are\", \"arent\", \"around\", \"as\", \"aside\", \"ask\", \"asking\", \"associated\", \"at\", \"available\", \"away\", \"awfully\", \"be\", \"became\", \"because\", \"become\", \"becomes\", \"becoming\", \"been\", \"before\", \"beforehand\", \"behind\", \"being\", \"believe\", \"below\", \"beside\", \"besides\", \"best\", \"better\", \"between\", \"beyond\", \"both\", \"brief\", \"but\", \"by\", \"cmon\", \"cs\", \"came\", \"can\", \"cant\", \"cannot\", \"cant\", \"cause\", \"causes\", \"certain\", \"certainly\", \"changes\", \"clearly\", \"co\", \"com\", \"come\", \"comes\", \"concerning\", \"consequently\", \"consider\", \"considering\", \"contain\", \"containing\", \"contains\", \"corresponding\", \"could\", \"couldnt\", \"course\", \"currently\", \"definitely\", \"described\", \"despite\", \"did\", \"didnt\", \"different\", \"do\", \"does\", \"doesnt\", \"doing\", \"dont\", \"done\", \"down\", \"downwards\", \"during\", \"each\", \"edu\", \"eg\", \"eight\", \"either\", \"else\", \"elsewhere\", \"enough\", \"entirely\", \"especially\", \"et\", \"etc\", \"even\", \"ever\", \"every\", \"everybody\", \"everyone\", \"everything\", \"everywhere\", \"ex\", \"exactly\", \"example\", \"except\", \"far\", \"few\", \"ff\", \"fifth\", \"first\", \"five\", \"followed\", \"following\", \"follows\", \"for\", \"former\", \"formerly\", \"forth\", \"four\", \"from\", \"further\", \"furthermore\", \"get\", \"gets\", \"getting\", \"given\", \"gives\", \"go\", \"goes\", \"going\", \"gone\", \"got\", \"gotten\", \"greetings\", \"had\", \"hadnt\", \"happens\", \"hardly\", \"has\", \"hasnt\", \"have\", \"havent\", \"having\", \"he\", \"hes\", \"hello\", \"help\", \"hence\", \"her\", \"here\", \"heres\", \"hereafter\", \"hereby\", \"herein\", \"hereupon\", \"hers\", \"herself\", \"hi\", \"him\", \"himself\", \"his\", \"hither\", \"hopefully\", \"how\", \"howbeit\", \"however\", \"i\", \"id\", \"ill\", \"im\", \"ive\", \"ie\", \"if\", \"ignored\", \"immediate\", \"in\", \"inasmuch\", \"inc\", \"indeed\", \"indicate\", \"indicated\", \"indicates\", \"inner\", \"insofar\", \"instead\", \"into\", \"inward\", \"is\", \"isnt\", \"it\", \"itd\", \"itll\", \"its\", \"its\", \"itself\", \"just\", \"keep\", \"keeps\", \"kept\", \"know\", \"knows\", \"known\", \"last\", \"lately\", \"later\", \"latter\", \"latterly\", \"least\", \"less\", \"lest\", \"let\", \"lets\", \"like\", \"liked\", \"likely\", \"little\", \"look\", \"looking\", \"looks\", \"ltd\", \"mainly\", \"many\", \"may\", \"maybe\", \"me\", \"mean\", \"meanwhile\", \"merely\", \"might\", \"more\", \"moreover\", \"most\", \"mostly\", \"much\", \"must\", \"my\", \"myself\", \"name\", \"namely\", \"nd\", \"near\", \"nearly\", \"necessary\", \"need\", \"needs\", \"neither\", \"never\", \"nevertheless\", \"new\", \"next\", \"nine\", \"no\", \"nobody\", \"non\", \"none\", \"noone\", \"nor\", \"normally\", \"not\", \"nothing\", \"novel\", \"now\", \"nowhere\", \"obviously\", \"of\", \"off\", \"often\", \"oh\", \"ok\", \"okay\", \"old\", \"on\", \"once\", \"one\", \"ones\", \"only\", \"onto\", \"or\", \"other\", \"others\", \"otherwise\", \"ought\", \"our\", \"ours\", \"ourselves\", \"out\", \"outside\", \"over\", \"overall\", \"own\", \"particular\", \"particularly\", \"per\", \"perhaps\", \"placed\", \"please\", \"plus\", \"possible\", \"presumably\", \"probably\", \"provides\", \"que\", \"quite\", \"qv\", \"rather\", \"rd\", \"re\", \"really\", \"reasonably\", \"regarding\", \"regardless\", \"regards\", \"relatively\", \"respectively\", \"right\", \"said\", \"same\", \"saw\", \"say\", \"saying\", \"says\", \"second\", \"secondly\", \"see\", \"seeing\", \"seem\", \"seemed\", \"seeming\", \"seems\", \"seen\", \"self\", \"selves\", \"sensible\", \"sent\", \"serious\", \"seriously\", \"seven\", \"several\", \"shall\", \"she\", \"should\", \"shouldnt\", \"since\", \"six\", \"so\", \"some\", \"somebody\", \"somehow\", \"someone\", \"something\", \"sometime\", \"sometimes\", \"somewhat\", \"somewhere\", \"soon\", \"sorry\", \"specified\", \"specify\", \"specifying\", \"still\", \"sub\", \"such\", \"sup\", \"sure\", \"ts\", \"take\", \"taken\", \"tell\", \"tends\", \"th\", \"than\", \"thank\", \"thanks\", \"thanx\", \"that\", \"thats\", \"thats\", \"the\", \"their\", \"theirs\", \"them\", \"themselves\", \"then\", \"thence\", \"there\", \"theres\", \"thereafter\", \"thereby\", \"therefore\", \"therein\", \"theres\", \"thereupon\", \"these\", \"they\", \"theyd\", \"theyll\", \"theyre\", \"theyve\", \"think\", \"third\", \"this\", \"thorough\", \"thoroughly\", \"those\", \"though\", \"three\", \"through\", \"throughout\", \"thru\", \"thus\", \"to\", \"together\", \"too\", \"took\", \"toward\", \"towards\", \"tried\", \"tries\", \"truly\", \"try\", \"trying\", \"twice\", \"two\", \"un\", \"under\", \"unfortunately\", \"unless\", \"unlikely\", \"until\", \"unto\", \"up\", \"upon\", \"us\", \"use\", \"used\", \"useful\", \"uses\", \"using\", \"usually\", \"value\", \"various\", \"very\", \"via\", \"viz\", \"vs\", \"want\", \"wants\", \"was\", \"wasnt\", \"way\", \"we\", \"wed\", \"well\", \"were\", \"weve\", \"welcome\", \"well\", \"went\", \"were\", \"werent\", \"what\", \"whats\", \"whatever\", \"when\", \"whence\", \"whenever\", \"where\", \"wheres\", \"whereafter\", \"whereas\", \"whereby\", \"wherein\", \"whereupon\", \"wherever\", \"whether\", \"which\", \"while\", \"whither\", \"who\", \"whos\", \"whoever\", \"whole\", \"whom\", \"whose\", \"why\", \"will\", \"willing\", \"wish\", \"with\", \"within\", \"without\", \"wont\", \"wonder\", \"would\", \"would\", \"wouldnt\", \"yes\", \"yet\", \"you\", \"youd\", \"youll\", \"youre\", \"youve\", \"your\", \"yours\", \"yourself\", \"yourselves\", \"zero\"]\n",
    "stopwordsRemover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\").setStopWords(add_stopwords)\n",
    "\n",
    "# bag of words count\n",
    "countVectors = CountVectorizer(inputCol=\"filtered\", outputCol=\"features\", vocabSize=10000, minDF=5)\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler\n",
    "\n",
    "label_stringIdx = StringIndexer(inputCol = \"category\", outputCol = \"label\")\n",
    "pipeline = Pipeline(stages=[regexTokenizer, stopwordsRemover, countVectors, label_stringIdx])\n",
    "\n",
    "# Fit the pipeline to training documents.\n",
    "pipelineFit = pipeline.fit(trainingData)\n",
    "trainingDataSet = pipelineFit.transform(trainingData)\n",
    "testDataSet = pipelineFit.transform(testData)\n",
    "\n",
    "#dataset.show(200)\n",
    "\n",
    "# set seed for reproducibility\n",
    "#(trainingData1, testData1) = trainingDataSet.randomSplit([0.8, 0.2], seed = 100)\n",
    "print(\"Training Dataset Count: \" + str(trainingDataSet.count()))\n",
    "print(\"Test Dataset Count: \" + str(testDataSet.count()))\n",
    "\n",
    "#trainingDataSet.show(trainingDataSet.count())\n",
    "#testDataSet.show(testDataSet.count())\n",
    "\n",
    "lr = LogisticRegression(maxIter=20, regParam=0.3, elasticNetParam=0)\n",
    "lrModel = lr.fit(trainingDataSet)\n",
    "predictions = lrModel.transform(testDataSet)\n",
    "#predictions.show(50)\n",
    "predictions.filter(predictions['prediction'] == 0).select(\"text\",\"category\",\"probability\",\"label\",\"prediction\").orderBy(\"probability\", ascending=False).show(n = 10, truncate = 30)\n",
    "\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------+----------+------------------------------+-----+----------+\n",
      "|                          text|  category|                   probability|label|prediction|\n",
      "+------------------------------+----------+------------------------------+-----+----------+\n",
      "|There are now more ways to ...|blockchain|[0.9327160135051216,0.01505...|  0.0|       0.0|\n",
      "|The internet was born in te...|blockchain|[0.9327160135051216,0.01505...|  0.0|       0.0|\n",
      "|Instead of perfectly polish...|blockchain|[0.9325748691113527,0.01514...|  0.0|       0.0|\n",
      "|The pioneers of social medi...|blockchain|[0.9287856437823444,0.01562...|  0.0|       0.0|\n",
      "|The distance between the Wh...|blockchain|[0.9286906722930692,0.01688...|  0.0|       0.0|\n",
      "|Major companies have been f...|blockchain|[0.9273480740926981,0.01699...|  0.0|       0.0|\n",
      "|AdvertisementSupported byTh...|blockchain|[0.8003762420295648,0.04049...|  0.0|       0.0|\n",
      "|AdvertisementSupported byTO...|  business|[0.6228966542210546,0.22361...|  1.0|       0.0|\n",
      "|AdvertisementSupported byOp...|  business|[0.5478129580434932,0.22342...|  1.0|       0.0|\n",
      "|AdvertisementSupported byWA...|  business|[0.5016512673417965,0.28341...|  1.0|       0.0|\n",
      "+------------------------------+----------+------------------------------+-----+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7450501253132833"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Logistic Regression using TF-IDF Features\n",
    "\n",
    "from pyspark.ml.feature import HashingTF, IDF\n",
    "hashingTF = HashingTF(inputCol=\"filtered\", outputCol=\"rawFeatures\", numFeatures=10000)\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\", minDocFreq=5) #minDocFreq: remove sparse terms\n",
    "pipeline = Pipeline(stages=[regexTokenizer, stopwordsRemover, hashingTF, idf, label_stringIdx])\n",
    "pipelineFit = pipeline.fit(trainingData)\n",
    "trainingDataSet = pipelineFit.transform(trainingData)\n",
    "testDataSet = pipelineFit.transform(testData)\n",
    "#(trainingData, testData) = dataset.randomSplit([0.8, 0.2], seed = 100)\n",
    "lr = LogisticRegression(maxIter=20, regParam=0.3, elasticNetParam=0)\n",
    "lrModel = lr.fit(trainingDataSet)\n",
    "predictions = lrModel.transform(testDataSet)\n",
    "predictions.filter(predictions['prediction'] == 0).select(\"text\",\"category\",\"probability\",\"label\",\"prediction\").orderBy(\"probability\", ascending=False).show(n = 10, truncate = 30)\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "evaluator.evaluate(predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8748120300751882"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cross-Validation\n",
    "\n",
    "pipeline = Pipeline(stages=[regexTokenizer, stopwordsRemover, countVectors, label_stringIdx])\n",
    "pipelineFit = pipeline.fit(trainingData)\n",
    "trainingDataSet = pipelineFit.transform(trainingData)\n",
    "testDataSet = pipelineFit.transform(testData)\n",
    "#(trainingData, testData) = dataset.randomSplit([0.8, 0.2], seed = 100)\n",
    "lr = LogisticRegression(maxIter=20, regParam=0.3, elasticNetParam=0)\n",
    "\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "# Create ParamGrid for Cross Validation\n",
    "paramGrid = (ParamGridBuilder()\n",
    "             .addGrid(lr.regParam, [0.1, 0.3, 0.5]) # regularization parameter\n",
    "             .addGrid(lr.elasticNetParam, [0.0, 0.1, 0.2]) # Elastic Net Parameter (Ridge = 0)\n",
    "#            .addGrid(model.maxIter, [10, 20, 50]) #Number of iterations\n",
    "#            .addGrid(idf.numFeatures, [10, 100, 1000]) # Number of features\n",
    "             .build())\n",
    "# Create 5-fold CrossValidator\n",
    "cv = CrossValidator(estimator=lr, estimatorParamMaps=paramGrid, evaluator=evaluator, numFolds=5)\n",
    "cvModel = cv.fit(trainingDataSet)\n",
    "\n",
    "predictions = cvModel.transform(testDataSet)\n",
    "# Evaluate best model\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------+----------+------------------------------+-----+----------+\n",
      "|                          text|  category|                   probability|label|prediction|\n",
      "+------------------------------+----------+------------------------------+-----+----------+\n",
      "|AdvertisementSupported byTO...|  business|[1.0,1.0810095835387586E-16...|  1.0|       0.0|\n",
      "|AdvertisementWith Andrew Ro...|blockchain|[1.0,1.6549201130872253E-21...|  0.0|       0.0|\n",
      "|There are now more ways to ...|blockchain|[1.0,7.13836643613608E-33,1...|  0.0|       0.0|\n",
      "|Instead of perfectly polish...|blockchain|[1.0,7.13836643613608E-33,1...|  0.0|       0.0|\n",
      "|The internet was born in te...|blockchain|[1.0,7.13836643613608E-33,1...|  0.0|       0.0|\n",
      "|The pioneers of social medi...|blockchain|[1.0,7.13836643613608E-33,1...|  0.0|       0.0|\n",
      "|The distance between the Wh...|blockchain|[1.0,7.13836643613608E-33,1...|  0.0|       0.0|\n",
      "|Major companies have been f...|blockchain|[1.0,6.823462574461964E-33,...|  0.0|       0.0|\n",
      "|AdvertisementSupported byTh...|blockchain|[1.0,1.962470692656096E-45,...|  0.0|       0.0|\n",
      "|AdvertisementSupported byOp...|  business|[0.9906589951739553,0.00932...|  1.0|       0.0|\n",
      "+------------------------------+----------+------------------------------+-----+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.800598086124402"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Naive Bayes\n",
    "from pyspark.ml.classification import NaiveBayes\n",
    "nb = NaiveBayes(smoothing=1)\n",
    "model = nb.fit(trainingDataSet)\n",
    "predictions = model.transform(testDataSet)\n",
    "predictions.filter(predictions['prediction'] == 0).select(\"text\",\"category\",\"probability\",\"label\",\"prediction\").orderBy(\"probability\", ascending=False).show(n = 10, truncate = 30)\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------+----------+------------------------------+-----+----------+\n",
      "|                          text|  category|                   probability|label|prediction|\n",
      "+------------------------------+----------+------------------------------+-----+----------+\n",
      "|AdvertisementWith Andrew Ro...|blockchain|[0.36567863609579904,0.2930...|  0.0|       0.0|\n",
      "|Major companies have been f...|blockchain|[0.33481133575670435,0.2348...|  0.0|       0.0|\n",
      "|Instead of perfectly polish...|blockchain|[0.33481133575670435,0.2348...|  0.0|       0.0|\n",
      "|The distance between the Wh...|blockchain|[0.33481133575670435,0.2348...|  0.0|       0.0|\n",
      "|There are now more ways to ...|blockchain|[0.33481133575670435,0.2348...|  0.0|       0.0|\n",
      "|The internet was born in te...|blockchain|[0.33481133575670435,0.2348...|  0.0|       0.0|\n",
      "|The pioneers of social medi...|blockchain|[0.33481133575670435,0.2348...|  0.0|       0.0|\n",
      "|AdvertisementSupported byTh...|blockchain|[0.30819655659471196,0.2625...|  0.0|       0.0|\n",
      "|A proud father A brilliant ...|blockchain|[0.3066275964795107,0.25873...|  0.0|       0.0|\n",
      "+------------------------------+----------+------------------------------+-----+----------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9261278195488722"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Random Forest\n",
    "\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "rf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\", numTrees = 100, maxDepth = 4, maxBins = 32)\n",
    "# Train model with Training Data\n",
    "rfModel = rf.fit(trainingDataSet)\n",
    "predictions = rfModel.transform(testDataSet)\n",
    "predictions.filter(predictions['prediction'] == 0).select(\"text\",\"category\",\"probability\",\"label\",\"prediction\").orderBy(\"probability\", ascending=False).show(n = 10, truncate = 30)\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
