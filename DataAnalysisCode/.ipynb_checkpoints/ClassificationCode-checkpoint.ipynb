{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code does Multi-class text classification using pyspark.\n",
    "Article text is preprocessed by removing non-alphanumeric characters. \n",
    "Tokenizing, Stopwords removal and Count vector operations are done with a pipeline.\n",
    "The dataset is split into training and test sets to train the model and perform predictions.\n",
    "\n",
    "Predictions are made the following ways\n",
    "1. Logistic Regression using Count Vector Features\n",
    "2. Logistic Regression using TF-IDF Features\n",
    "3. Cross-Validation\n",
    "4. Naive Bayes\n",
    "5. Random Forest\n",
    "\n",
    "Sample output: Accuracy\n",
    "1. 0.73\n",
    "2. 0.76\n",
    "3. 0.76\n",
    "4. 0.80\n",
    "5. 0.78"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os, csv, re\n",
    "\n",
    "def preProcessData():\n",
    "    with open('ArticleData.csv', 'w+') as csvfile:\n",
    "        fieldnames = ['text', 'category']\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "\n",
    "        writer.writeheader()\n",
    "\n",
    "        keyWords = ['blockchain','business','sports','politics']\n",
    "\n",
    "        for keyWord in keyWords:\n",
    "\n",
    "            for i in range(0, 50):\n",
    "                total_path = \"/Users/sajidkhan/Desktop/DIC/Lab3/Input/\"+keyWord\n",
    "                text_file = open(os.path.join(total_path, keyWord+str(i)+\".txt\"), \"r\")\n",
    "                for line in text_file:\n",
    "                    line2 = re.sub('[^A-Za-z0-9\\s]+', '', line)\n",
    "                    writer.writerow({'text': line2, 'category': keyWord})  \n",
    "\n",
    "                text_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|  category|count|\n",
      "+----------+-----+\n",
      "|blockchain|   50|\n",
      "|  politics|   50|\n",
      "|    sports|   50|\n",
      "|  business|   50|\n",
      "+----------+-----+\n",
      "\n",
      "Training Dataset Count: 161\n",
      "Test Dataset Count: 39\n",
      "+------------------------------+----------+------------------------------+-----+----------+\n",
      "|                          text|  category|                   probability|label|prediction|\n",
      "+------------------------------+----------+------------------------------+-----+----------+\n",
      "|AdvertisementSupported byBE...|blockchain|[0.9794388072609735,0.00956...|  0.0|       0.0|\n",
      "|AdvertisementSupported byJA...|blockchain|[0.9717340764936586,0.01016...|  0.0|       0.0|\n",
      "|AdvertisementSupported byBr...|blockchain|[0.6992683853363467,0.14656...|  0.0|       0.0|\n",
      "|AdvertisementSupported byLe...|blockchain|[0.6864717614051721,0.07115...|  0.0|       0.0|\n",
      "|AdvertisementSupported byPA...|blockchain|[0.5258271593659443,0.20536...|  0.0|       0.0|\n",
      "|AdvertisementSupported byWe...|    sports|[0.4750221187151098,0.17989...|  3.0|       0.0|\n",
      "|AdvertisementSupported byTh...|blockchain|[0.4589111860381329,0.10178...|  0.0|       0.0|\n",
      "|AdvertisementSupported byAt...|  business|[0.42614149379049965,0.3314...|  1.0|       0.0|\n",
      "|AdvertisementSupported byHe...|  business|[0.42389222466062293,0.1904...|  1.0|       0.0|\n",
      "|AdvertisementSupported byPA...|  business|[0.40165606460248293,0.3009...|  1.0|       0.0|\n",
      "+------------------------------+----------+------------------------------+-----+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7315664727429434"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model Training and Evaluation\n",
    "# Logistic Regression using Count Vector Features\n",
    "\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark import SparkContext\n",
    "import pyspark\n",
    "import pandas as pd\n",
    "\n",
    "sc = pyspark.SparkContext.getOrCreate()\n",
    "sqlContext = SQLContext(sc)\n",
    "#preProcessData() run only once..\n",
    "dataFile = \"/Users/sajidkhan/Desktop/DIC/Lab3/DataAnalysisCode/ArticleData.csv\"\n",
    "data = sqlContext.read.format('com.databricks.spark.csv').options(header='true', inferschema='true').load(dataFile)\n",
    "\n",
    "#data.show(5)\n",
    "\n",
    "#data.printSchema()\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "data.groupBy(\"category\").count().orderBy(col(\"count\").desc()).show()\n",
    "\n",
    "from pyspark.ml.feature import RegexTokenizer, StopWordsRemover, CountVectorizer\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# regular expression tokenizer\n",
    "regexTokenizer = RegexTokenizer(inputCol=\"text\", outputCol=\"words\", pattern=\"\\\\W\")\n",
    "\n",
    "# stop words\n",
    "add_stopwords = [\"a\", \"an\", \"as\", \"able\", \"about\", \"above\", \"according\", \"accordingly\", \"across\", \"actually\", \"after\", \"afterwards\", \"again\", \"against\", \"aint\", \"all\", \"allow\", \"allows\", \"almost\", \"alone\", \"along\", \"already\", \"also\", \"although\", \"always\", \"am\", \"among\", \"amongst\", \"an\", \"and\", \"another\", \"any\", \"anybody\", \"anyhow\", \"anyone\", \"anything\", \"anyway\", \"anyways\", \"anywhere\", \"apart\", \"appear\", \"appreciate\", \"appropriate\", \"are\", \"arent\", \"around\", \"as\", \"aside\", \"ask\", \"asking\", \"associated\", \"at\", \"available\", \"away\", \"awfully\", \"be\", \"became\", \"because\", \"become\", \"becomes\", \"becoming\", \"been\", \"before\", \"beforehand\", \"behind\", \"being\", \"believe\", \"below\", \"beside\", \"besides\", \"best\", \"better\", \"between\", \"beyond\", \"both\", \"brief\", \"but\", \"by\", \"cmon\", \"cs\", \"came\", \"can\", \"cant\", \"cannot\", \"cant\", \"cause\", \"causes\", \"certain\", \"certainly\", \"changes\", \"clearly\", \"co\", \"com\", \"come\", \"comes\", \"concerning\", \"consequently\", \"consider\", \"considering\", \"contain\", \"containing\", \"contains\", \"corresponding\", \"could\", \"couldnt\", \"course\", \"currently\", \"definitely\", \"described\", \"despite\", \"did\", \"didnt\", \"different\", \"do\", \"does\", \"doesnt\", \"doing\", \"dont\", \"done\", \"down\", \"downwards\", \"during\", \"each\", \"edu\", \"eg\", \"eight\", \"either\", \"else\", \"elsewhere\", \"enough\", \"entirely\", \"especially\", \"et\", \"etc\", \"even\", \"ever\", \"every\", \"everybody\", \"everyone\", \"everything\", \"everywhere\", \"ex\", \"exactly\", \"example\", \"except\", \"far\", \"few\", \"ff\", \"fifth\", \"first\", \"five\", \"followed\", \"following\", \"follows\", \"for\", \"former\", \"formerly\", \"forth\", \"four\", \"from\", \"further\", \"furthermore\", \"get\", \"gets\", \"getting\", \"given\", \"gives\", \"go\", \"goes\", \"going\", \"gone\", \"got\", \"gotten\", \"greetings\", \"had\", \"hadnt\", \"happens\", \"hardly\", \"has\", \"hasnt\", \"have\", \"havent\", \"having\", \"he\", \"hes\", \"hello\", \"help\", \"hence\", \"her\", \"here\", \"heres\", \"hereafter\", \"hereby\", \"herein\", \"hereupon\", \"hers\", \"herself\", \"hi\", \"him\", \"himself\", \"his\", \"hither\", \"hopefully\", \"how\", \"howbeit\", \"however\", \"i\", \"id\", \"ill\", \"im\", \"ive\", \"ie\", \"if\", \"ignored\", \"immediate\", \"in\", \"inasmuch\", \"inc\", \"indeed\", \"indicate\", \"indicated\", \"indicates\", \"inner\", \"insofar\", \"instead\", \"into\", \"inward\", \"is\", \"isnt\", \"it\", \"itd\", \"itll\", \"its\", \"its\", \"itself\", \"just\", \"keep\", \"keeps\", \"kept\", \"know\", \"knows\", \"known\", \"last\", \"lately\", \"later\", \"latter\", \"latterly\", \"least\", \"less\", \"lest\", \"let\", \"lets\", \"like\", \"liked\", \"likely\", \"little\", \"look\", \"looking\", \"looks\", \"ltd\", \"mainly\", \"many\", \"may\", \"maybe\", \"me\", \"mean\", \"meanwhile\", \"merely\", \"might\", \"more\", \"moreover\", \"most\", \"mostly\", \"much\", \"must\", \"my\", \"myself\", \"name\", \"namely\", \"nd\", \"near\", \"nearly\", \"necessary\", \"need\", \"needs\", \"neither\", \"never\", \"nevertheless\", \"new\", \"next\", \"nine\", \"no\", \"nobody\", \"non\", \"none\", \"noone\", \"nor\", \"normally\", \"not\", \"nothing\", \"novel\", \"now\", \"nowhere\", \"obviously\", \"of\", \"off\", \"often\", \"oh\", \"ok\", \"okay\", \"old\", \"on\", \"once\", \"one\", \"ones\", \"only\", \"onto\", \"or\", \"other\", \"others\", \"otherwise\", \"ought\", \"our\", \"ours\", \"ourselves\", \"out\", \"outside\", \"over\", \"overall\", \"own\", \"particular\", \"particularly\", \"per\", \"perhaps\", \"placed\", \"please\", \"plus\", \"possible\", \"presumably\", \"probably\", \"provides\", \"que\", \"quite\", \"qv\", \"rather\", \"rd\", \"re\", \"really\", \"reasonably\", \"regarding\", \"regardless\", \"regards\", \"relatively\", \"respectively\", \"right\", \"said\", \"same\", \"saw\", \"say\", \"saying\", \"says\", \"second\", \"secondly\", \"see\", \"seeing\", \"seem\", \"seemed\", \"seeming\", \"seems\", \"seen\", \"self\", \"selves\", \"sensible\", \"sent\", \"serious\", \"seriously\", \"seven\", \"several\", \"shall\", \"she\", \"should\", \"shouldnt\", \"since\", \"six\", \"so\", \"some\", \"somebody\", \"somehow\", \"someone\", \"something\", \"sometime\", \"sometimes\", \"somewhat\", \"somewhere\", \"soon\", \"sorry\", \"specified\", \"specify\", \"specifying\", \"still\", \"sub\", \"such\", \"sup\", \"sure\", \"ts\", \"take\", \"taken\", \"tell\", \"tends\", \"th\", \"than\", \"thank\", \"thanks\", \"thanx\", \"that\", \"thats\", \"thats\", \"the\", \"their\", \"theirs\", \"them\", \"themselves\", \"then\", \"thence\", \"there\", \"theres\", \"thereafter\", \"thereby\", \"therefore\", \"therein\", \"theres\", \"thereupon\", \"these\", \"they\", \"theyd\", \"theyll\", \"theyre\", \"theyve\", \"think\", \"third\", \"this\", \"thorough\", \"thoroughly\", \"those\", \"though\", \"three\", \"through\", \"throughout\", \"thru\", \"thus\", \"to\", \"together\", \"too\", \"took\", \"toward\", \"towards\", \"tried\", \"tries\", \"truly\", \"try\", \"trying\", \"twice\", \"two\", \"un\", \"under\", \"unfortunately\", \"unless\", \"unlikely\", \"until\", \"unto\", \"up\", \"upon\", \"us\", \"use\", \"used\", \"useful\", \"uses\", \"using\", \"usually\", \"value\", \"various\", \"very\", \"via\", \"viz\", \"vs\", \"want\", \"wants\", \"was\", \"wasnt\", \"way\", \"we\", \"wed\", \"well\", \"were\", \"weve\", \"welcome\", \"well\", \"went\", \"were\", \"werent\", \"what\", \"whats\", \"whatever\", \"when\", \"whence\", \"whenever\", \"where\", \"wheres\", \"whereafter\", \"whereas\", \"whereby\", \"wherein\", \"whereupon\", \"wherever\", \"whether\", \"which\", \"while\", \"whither\", \"who\", \"whos\", \"whoever\", \"whole\", \"whom\", \"whose\", \"why\", \"will\", \"willing\", \"wish\", \"with\", \"within\", \"without\", \"wont\", \"wonder\", \"would\", \"would\", \"wouldnt\", \"yes\", \"yet\", \"you\", \"youd\", \"youll\", \"youre\", \"youve\", \"your\", \"yours\", \"yourself\", \"yourselves\", \"zero\"]\n",
    "stopwordsRemover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\").setStopWords(add_stopwords)\n",
    "\n",
    "# bag of words count\n",
    "countVectors = CountVectorizer(inputCol=\"filtered\", outputCol=\"features\", vocabSize=10000, minDF=5)\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler\n",
    "\n",
    "label_stringIdx = StringIndexer(inputCol = \"category\", outputCol = \"label\")\n",
    "pipeline = Pipeline(stages=[regexTokenizer, stopwordsRemover, countVectors, label_stringIdx])\n",
    "\n",
    "# Fit the pipeline to training documents.\n",
    "pipelineFit = pipeline.fit(data)\n",
    "dataset = pipelineFit.transform(data)\n",
    "#dataset.show(200)\n",
    "\n",
    "# set seed for reproducibility\n",
    "(trainingData, testData) = dataset.randomSplit([0.8, 0.2], seed = 100)\n",
    "print(\"Training Dataset Count: \" + str(trainingData.count()))\n",
    "print(\"Test Dataset Count: \" + str(testData.count()))\n",
    "\n",
    "lr = LogisticRegression(maxIter=20, regParam=0.3, elasticNetParam=0)\n",
    "lrModel = lr.fit(trainingData)\n",
    "predictions = lrModel.transform(testData)\n",
    "predictions.filter(predictions['prediction'] == 0).select(\"text\",\"category\",\"probability\",\"label\",\"prediction\").orderBy(\"probability\", ascending=False).show(n = 10, truncate = 30)\n",
    "\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------+----------+------------------------------+-----+----------+\n",
      "|                          text|  category|                   probability|label|prediction|\n",
      "+------------------------------+----------+------------------------------+-----+----------+\n",
      "|AdvertisementSupported byBE...|blockchain|[0.9774984551184397,0.01178...|  0.0|       0.0|\n",
      "|AdvertisementSupported byJA...|blockchain|[0.9724146117374186,0.01193...|  0.0|       0.0|\n",
      "|AdvertisementSupported byBr...|blockchain|[0.6603851970428619,0.16771...|  0.0|       0.0|\n",
      "|AdvertisementSupported byLe...|blockchain|[0.5224822642505726,0.08110...|  0.0|       0.0|\n",
      "|AdvertisementSupported byPA...|blockchain|[0.5016705322088901,0.14088...|  0.0|       0.0|\n",
      "|AdvertisementSupported byOp...|blockchain|[0.47397423413169515,0.0783...|  0.0|       0.0|\n",
      "|AdvertisementSupported byPA...|  business|[0.43361068525024304,0.2497...|  1.0|       0.0|\n",
      "|AdvertisementSupported byWe...|    sports|[0.4001291260953632,0.15227...|  3.0|       0.0|\n",
      "|AdvertisementSupported byTh...|blockchain|[0.3985941600417048,0.14402...|  0.0|       0.0|\n",
      "|AdvertisementSupported byAd...|blockchain|[0.34219611173184267,0.2258...|  0.0|       0.0|\n",
      "+------------------------------+----------+------------------------------+-----+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7641025641025642"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Logistic Regression using TF-IDF Features\n",
    "\n",
    "from pyspark.ml.feature import HashingTF, IDF\n",
    "hashingTF = HashingTF(inputCol=\"filtered\", outputCol=\"rawFeatures\", numFeatures=10000)\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\", minDocFreq=5) #minDocFreq: remove sparse terms\n",
    "pipeline = Pipeline(stages=[regexTokenizer, stopwordsRemover, hashingTF, idf, label_stringIdx])\n",
    "pipelineFit = pipeline.fit(data)\n",
    "dataset = pipelineFit.transform(data)\n",
    "(trainingData, testData) = dataset.randomSplit([0.8, 0.2], seed = 100)\n",
    "lr = LogisticRegression(maxIter=20, regParam=0.3, elasticNetParam=0)\n",
    "lrModel = lr.fit(trainingData)\n",
    "predictions = lrModel.transform(testData)\n",
    "predictions.filter(predictions['prediction'] == 0).select(\"text\",\"category\",\"probability\",\"label\",\"prediction\").orderBy(\"probability\", ascending=False).show(n = 10, truncate = 30)\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "evaluator.evaluate(predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7573001508295625"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cross-Validation\n",
    "\n",
    "pipeline = Pipeline(stages=[regexTokenizer, stopwordsRemover, countVectors, label_stringIdx])\n",
    "pipelineFit = pipeline.fit(data)\n",
    "dataset = pipelineFit.transform(data)\n",
    "(trainingData, testData) = dataset.randomSplit([0.8, 0.2], seed = 100)\n",
    "lr = LogisticRegression(maxIter=20, regParam=0.3, elasticNetParam=0)\n",
    "\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "# Create ParamGrid for Cross Validation\n",
    "paramGrid = (ParamGridBuilder()\n",
    "             .addGrid(lr.regParam, [0.1, 0.3, 0.5]) # regularization parameter\n",
    "             .addGrid(lr.elasticNetParam, [0.0, 0.1, 0.2]) # Elastic Net Parameter (Ridge = 0)\n",
    "#            .addGrid(model.maxIter, [10, 20, 50]) #Number of iterations\n",
    "#            .addGrid(idf.numFeatures, [10, 100, 1000]) # Number of features\n",
    "             .build())\n",
    "# Create 5-fold CrossValidator\n",
    "cv = CrossValidator(estimator=lr, estimatorParamMaps=paramGrid, evaluator=evaluator, numFolds=5)\n",
    "cvModel = cv.fit(trainingData)\n",
    "\n",
    "predictions = cvModel.transform(testData)\n",
    "# Evaluate best model\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "evaluator.evaluate(predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------+----------+------------------------------+-----+----------+\n",
      "|                          text|  category|                   probability|label|prediction|\n",
      "+------------------------------+----------+------------------------------+-----+----------+\n",
      "|AdvertisementSupported byBr...|blockchain|[1.0,3.891326381433641E-25,...|  0.0|       0.0|\n",
      "|AdvertisementSupported byPA...|blockchain|[1.0,7.034308272942E-49,1.8...|  0.0|       0.0|\n",
      "|AdvertisementSupported byJA...|blockchain|[1.0,5.937137900977191E-83,...|  0.0|       0.0|\n",
      "|AdvertisementSupported byBE...|blockchain|[1.0,3.021747213129572E-127...|  0.0|       0.0|\n",
      "|AdvertisementSupported byLe...|blockchain|[0.9999791333172087,6.54399...|  0.0|       0.0|\n",
      "|AdvertisementSupported byTh...|blockchain|[0.9996263473745822,6.98917...|  0.0|       0.0|\n",
      "|AdvertisementSupported byco...|blockchain|[0.9666104240012462,2.68553...|  0.0|       0.0|\n",
      "+------------------------------+----------+------------------------------+-----+----------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7956083018002522"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Naive Bayes\n",
    "from pyspark.ml.classification import NaiveBayes\n",
    "nb = NaiveBayes(smoothing=1)\n",
    "model = nb.fit(trainingData)\n",
    "predictions = model.transform(testData)\n",
    "predictions.filter(predictions['prediction'] == 0).select(\"text\",\"category\",\"probability\",\"label\",\"prediction\").orderBy(\"probability\", ascending=False).show(n = 10, truncate = 30)\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------+----------+------------------------------+-----+----------+\n",
      "|                          text|  category|                   probability|label|prediction|\n",
      "+------------------------------+----------+------------------------------+-----+----------+\n",
      "|AdvertisementSupported byBE...|blockchain|[0.5384341125599904,0.18089...|  0.0|       0.0|\n",
      "|AdvertisementSupported byJA...|blockchain|[0.48177160890257,0.1976802...|  0.0|       0.0|\n",
      "|AdvertisementSupported byPA...|blockchain|[0.4800704086492222,0.23724...|  0.0|       0.0|\n",
      "|AdvertisementSupported byBr...|blockchain|[0.36741608460541303,0.3074...|  0.0|       0.0|\n",
      "|AdvertisementSupported byco...|blockchain|[0.3308526107187717,0.23151...|  0.0|       0.0|\n",
      "|AdvertisementSupported byLe...|blockchain|[0.3272008595451493,0.22196...|  0.0|       0.0|\n",
      "|AdvertisementSupported byOp...|blockchain|[0.3128171015215361,0.22418...|  0.0|       0.0|\n",
      "|AdvertisementSupported byTh...|blockchain|[0.3088612965950106,0.20622...|  0.0|       0.0|\n",
      "+------------------------------+----------+------------------------------+-----+----------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7782916740010563"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Random Forest\n",
    "\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "rf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\", numTrees = 100, maxDepth = 4, maxBins = 32)\n",
    "# Train model with Training Data\n",
    "rfModel = rf.fit(trainingData)\n",
    "predictions = rfModel.transform(testData)\n",
    "predictions.filter(predictions['prediction'] == 0).select(\"text\",\"category\",\"probability\",\"label\",\"prediction\").orderBy(\"probability\", ascending=False).show(n = 10, truncate = 30)\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
