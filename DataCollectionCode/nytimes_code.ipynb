{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following example collects NYTimes Articles for different topics like Sports, Business, Politics and Technology. sports, business, politics and blockchain are the respective keywords used. \n",
    "We collected total of 200 articles spanning 50 articles per topic.\n",
    "We also collected 10 more articles from each topic to validate unknown dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nytimesarticle import articleAPI\n",
    "api = articleAPI('8b18491cab36479486581b2c19309a5e')\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import os\n",
    "\n",
    "def collectData(keyWord):\n",
    "    i = 0\n",
    "    for x in range(0, 15):\n",
    "        \n",
    "        if i==60:\n",
    "            break\n",
    "        articles = api.search( q = keyWord, begin_date = '20140101', page=x)\n",
    "        #print articles\n",
    "\n",
    "        urls = []\n",
    "        for article in articles['response']['docs']:\n",
    "                a = {}\n",
    "                a['url'] = article['web_url']\n",
    "                urls.append(a)\n",
    "        print urls\n",
    "\n",
    "        # f = open(\"soccer.txt\", \"a+\")\n",
    "        # f.close()\n",
    "        \n",
    "        if not os.path.exists(keyWord):\n",
    "            os.makedirs(keyWord)\n",
    "\n",
    "        for item in urls:\n",
    "            url = item['url']\n",
    "            session = requests.Session()\n",
    "            req = requests.get(url)\n",
    "            soup = BeautifulSoup(req.text)\n",
    "            paragraphs = soup.find_all('p', text=True)\n",
    "\n",
    "            article_text = \"\"\n",
    "            for p in paragraphs:\n",
    "                article_text = article_text + p.get_text()\n",
    "                print str(\"article_text: \")+article_text\n",
    "\n",
    "            if article_text != \"\":\n",
    "                \n",
    "                text_file = open(os.path.join(keyWord, keyWord+str(i)+\".txt\"), \"a+\")\n",
    "                text_file.write(\"%s\" % article_text.encode(\"UTF-8\"))\n",
    "                text_file.close()\n",
    "                i = i+1\n",
    "\n",
    "# This code will collect data(50 files per topic) based on the keywords in the array \n",
    "# and save them in respective directory\n",
    "keyWords = ['sports','business','politics','blockchain']\n",
    "\n",
    "for word in keyWords:\n",
    "    collectData(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
